{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CUDA-BEVFusion Evaluation on nuScenes Dataset\n",
        "\n",
        "This notebook runs CUDA-BEVFusion inference on the nuScenes validation set and computes accuracy metrics (mAP and NDS).\n",
        "\n",
        "## Overview\n",
        "- **Model**: ResNet50 BEVFusion-Base\n",
        "- **Dataset**: nuScenes validation set\n",
        "- **Metrics**: mAP (mean Average Precision) and NDS (nuScenes Detection Score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Compute capability: {torch.cuda.get_device_capability(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install system dependencies\n",
        "import os\n",
        "os.system(\"apt-get update -qq\")\n",
        "os.system(\"apt-get install -y -qq libprotobuf-dev protobuf-compiler cmake build-essential git\")\n",
        "\n",
        "# Install Python dependencies\n",
        "os.system(\"pip install -q onnx==1.12.0 onnx_simplifier==0.4.8 onnxsim==0.4.10 onnxruntime==1.13.1\")\n",
        "os.system(\"pip install -q --extra-index-url https://pypi.ngc.nvidia.com onnx-graphsurgeon pytorch-quantization\")\n",
        "os.system(\"pip install -q shapely==1.8.0 pyyaml\")\n",
        "os.system(\"pip install -q mmcv-full mmdet mmdet3d nuscenes-devkit\")\n",
        "os.system(\"pip install -q torchpack\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone CUDA-BEVFusion Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone repository (adjust path if needed)\n",
        "repo_path = \"/content/CUDA-BEVFusion\"\n",
        "if not os.path.exists(repo_path):\n",
        "    os.system(f\"git clone --recursive https://github.com/NVIDIA-AI-IOT/CUDA-BEVFusion.git {repo_path}\")\n",
        "else:\n",
        "    print(f\"Repository already exists at {repo_path}\")\n",
        "\n",
        "os.chdir(repo_path)\n",
        "print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Models and Dataset\n",
        "\n",
        "**Note**: You'll need to download:\n",
        "1. **model.zip** from [NVBox](https://nvidia.box.com/shared/static/vc1ezra9kw7gu7wg3v8cwuiqshwr8b39) or [Baidu Drive](https://pan.baidu.com/s/1BiAoQ8L7nC45vEwkN3bSGQ?pwd=8jb6)\n",
        "2. **nuScenes dataset** - You need the full nuScenes validation set with annotations\n",
        "\n",
        "For Colab, you can upload these files or use gdown to download from Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install gdown for downloading from Google Drive\n",
        "os.system(\"pip install -q gdown\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "os.makedirs(\"data/nuscenes\", exist_ok=True)\n",
        "\n",
        "print(\"Please download the following files:\")\n",
        "print(\"1. model.zip - Extract to ./model/\")\n",
        "print(\"2. nuScenes dataset - Place in ./data/nuscenes/\")\n",
        "print(\"\\nYou can use the following commands:\")\n",
        "print(\"  - Upload files via Colab file browser\")\n",
        "print(\"  - Or use gdown if you have Google Drive links\")\n",
        "print(\"\\nExample:\")\n",
        "print(\"  os.system('gdown <file_id> -O model.zip')\")\n",
        "print(\"  os.system('unzip -q model.zip -d model/')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setup Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Detect CUDA version\n",
        "import subprocess\n",
        "result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "cuda_version = \"12.6\"  # Default, adjust based on your system\n",
        "if \"release 11\" in result.stdout:\n",
        "    cuda_version = \"11.4\"\n",
        "elif \"release 12\" in result.stdout:\n",
        "    cuda_version = \"12.6\"\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"DEBUG_MODEL\"] = \"resnet50\"\n",
        "os.environ[\"DEBUG_PRECISION\"] = \"fp16\"\n",
        "os.environ[\"USE_Python\"] = \"ON\"\n",
        "os.environ[\"SPCONV_CUDA_VERSION\"] = cuda_version\n",
        "\n",
        "# Detect compute capability\n",
        "if torch.cuda.is_available():\n",
        "    compute_cap = torch.cuda.get_device_capability(0)\n",
        "    os.environ[\"CUDASM\"] = f\"{compute_cap[0]}{compute_cap[1]}\"\n",
        "    print(f\"Compute capability: SM {compute_cap[0]}.{compute_cap[1]}\")\n",
        "else:\n",
        "    os.environ[\"CUDASM\"] = \"80\"  # Default to SM 8.0\n",
        "\n",
        "# CUDA paths (adjust for Colab)\n",
        "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda\"\n",
        "os.environ[\"CUDA_Inc\"] = f\"{os.environ['CUDA_HOME']}/include\"\n",
        "os.environ[\"CUDA_Lib\"] = f\"{os.environ['CUDA_HOME']}/lib64\"\n",
        "\n",
        "# TensorRT paths (adjust for Colab - may need to install TensorRT)\n",
        "# For Colab, TensorRT might be in /usr/lib/x86_64-linux-gnu or needs installation\n",
        "os.environ[\"TensorRT_Inc\"] = \"/usr/include/x86_64-linux-gnu\"\n",
        "os.environ[\"TensorRT_Lib\"] = \"/usr/lib/x86_64-linux-gnu\"\n",
        "os.environ[\"TensorRT_Bin\"] = \"/usr/bin\"\n",
        "\n",
        "print(f\"Model: {os.environ['DEBUG_MODEL']}\")\n",
        "print(f\"Precision: {os.environ['DEBUG_PRECISION']}\")\n",
        "print(f\"CUDA Version: {os.environ['SPCONV_CUDA_VERSION']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Build TensorRT Engines\n",
        "\n",
        "Build the TensorRT engines from ONNX models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if TensorRT engines already exist\n",
        "model_path = f\"model/{os.environ['DEBUG_MODEL']}/build\"\n",
        "if os.path.exists(model_path) and os.path.exists(f\"{model_path}/camera.backbone.plan\"):\n",
        "    print(f\"TensorRT engines already exist at {model_path}\")\n",
        "    print(\"Skipping build step. If you want to rebuild, delete the build directory.\")\n",
        "else:\n",
        "    print(\"Building TensorRT engines...\")\n",
        "    print(\"Note: This requires TensorRT to be installed and may take several minutes.\")\n",
        "    # Run build script\n",
        "    os.system(\"bash tool/build_trt_engine.sh\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Build Python Library\n",
        "\n",
        "Build the libpybev.so Python extension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate protobuf code\n",
        "os.system(\"bash src/onnx/make_pb.sh\")\n",
        "\n",
        "# Build the library\n",
        "print(\"Building Python library...\")\n",
        "os.system(\"bash tool/run.sh\")\n",
        "\n",
        "# Verify build\n",
        "build_path = \"build\"\n",
        "if os.path.exists(f\"{build_path}/libpybev.so\"):\n",
        "    print(f\"✓ Successfully built libpybev.so\")\n",
        "    # Add to Python path\n",
        "    sys.path.insert(0, os.path.abspath(build_path))\n",
        "    sys.path.insert(0, os.path.abspath(\"tool\"))\n",
        "    sys.path.insert(0, os.path.abspath(\"src/common\"))\n",
        "else:\n",
        "    print(\"✗ Build failed - libpybev.so not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Prepare nuScenes Dataset\n",
        "\n",
        "Ensure the nuScenes dataset is properly formatted with validation annotations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check nuScenes dataset structure\n",
        "nuscenes_root = \"data/nuscenes\"\n",
        "required_files = [\n",
        "    \"nuscenes_infos_val.pkl\",\n",
        "    \"samples\",\n",
        "    \"sweeps\",\n",
        "    \"maps\"\n",
        "]\n",
        "\n",
        "print(\"Checking nuScenes dataset structure...\")\n",
        "for item in required_files:\n",
        "    path = os.path.join(nuscenes_root, item)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"✓ Found: {item}\")\n",
        "    else:\n",
        "        print(f\"✗ Missing: {item}\")\n",
        "\n",
        "# If dataset converter is needed\n",
        "if not os.path.exists(os.path.join(nuscenes_root, \"nuscenes_infos_val.pkl\")):\n",
        "    print(\"\\nNote: You may need to run the dataset converter:\")\n",
        "    print(\"  python bevfusion/tools/create_data.py nuscenes --root-path data/nuscenes --out-dir data/nuscenes --version v1.0-mini\")\n",
        "    print(\"\\nFor full validation set, use v1.0-trainval\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Run Evaluation on nuScenes Validation Set\n",
        "\n",
        "This will run inference on all validation samples and compute mAP and NDS metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up paths for evaluation\n",
        "config_path = \"bevfusion/configs/nuscenes/det/transfusion/secfpn/camera+lidar/resnet50/convfuser.yaml\"\n",
        "\n",
        "# If config doesn't exist, try alternative path\n",
        "if not os.path.exists(config_path):\n",
        "    config_path = \"configs/nuscenes/det/transfusion/secfpn/camera+lidar/resnet50/convfuser.yaml\"\n",
        "\n",
        "if not os.path.exists(config_path):\n",
        "    print(f\"Config file not found at {config_path}\")\n",
        "    print(\"Please ensure the config file exists or update the path\")\n",
        "else:\n",
        "    print(f\"Using config: {config_path}\")\n",
        "\n",
        "# Update dataset root in config if needed\n",
        "try:\n",
        "    import yaml\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    print(f\"Dataset root in config: {config.get('dataset_root', 'not set')}\")\n",
        "except ImportError:\n",
        "    print(\"PyYAML not installed. Install with: pip install pyyaml\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not read config: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation using the test script\n",
        "# This will compute mAP and NDS on the validation set\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Change to repository root\n",
        "os.chdir(repo_path)\n",
        "\n",
        "# Run evaluation\n",
        "eval_script = \"qat/test-mAP-for-cuda.py\"\n",
        "if os.path.exists(eval_script):\n",
        "    print(\"Running evaluation...\")\n",
        "    print(\"This may take a while depending on dataset size...\")\n",
        "    \n",
        "    cmd = [\n",
        "        sys.executable, eval_script,\n",
        "        \"--config\", config_path,\n",
        "        \"--eval\", \"bbox\",\n",
        "        \"--out\", \"results.pkl\"\n",
        "    ]\n",
        "    \n",
        "    result = subprocess.run(cmd, capture_output=False, text=True)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n✓ Evaluation completed successfully!\")\n",
        "    else:\n",
        "        print(f\"\\n✗ Evaluation failed with return code {result.returncode}\")\n",
        "else:\n",
        "    print(f\"Evaluation script not found at {eval_script}\")\n",
        "    print(\"Please ensure the script exists in the repository\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Display Results\n",
        "\n",
        "Display the computed accuracy metrics (mAP and NDS).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if results file exists and display metrics\n",
        "results_file = \"results.pkl\"\n",
        "\n",
        "if os.path.exists(results_file):\n",
        "    import pickle\n",
        "    import json\n",
        "    \n",
        "    # Try to load results\n",
        "    try:\n",
        "        with open(results_file, 'rb') as f:\n",
        "            results = pickle.load(f)\n",
        "        print(f\"Loaded results from {results_file}\")\n",
        "        print(f\"Number of samples evaluated: {len(results)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading results: {e}\")\n",
        "    \n",
        "    # Check for metrics summary\n",
        "    metrics_file = \"metrics_summary.json\"\n",
        "    if os.path.exists(metrics_file):\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATION RESULTS - nuScenes Validation Set\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Display key metrics\n",
        "        if 'mean_ap' in metrics:\n",
        "            print(f\"\\nmAP (mean Average Precision): {metrics['mean_ap']:.4f}\")\n",
        "        if 'nd_score' in metrics:\n",
        "            print(f\"NDS (nuScenes Detection Score): {metrics['nd_score']:.4f}\")\n",
        "        \n",
        "        # Display per-class metrics if available\n",
        "        if 'mean_dist_aps' in metrics:\n",
        "            print(\"\\nPer-class mAP:\")\n",
        "            for class_name, ap in metrics['mean_dist_aps'].items():\n",
        "                print(f\"  {class_name}: {ap:.4f}\")\n",
        "        \n",
        "        # Display other metrics\n",
        "        print(\"\\nOther metrics:\")\n",
        "        for key, value in metrics.items():\n",
        "            if key not in ['mean_ap', 'nd_score', 'mean_dist_aps']:\n",
        "                if isinstance(value, (int, float)):\n",
        "                    print(f\"  {key}: {value:.4f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "        \n",
        "        print(\"=\"*60)\n",
        "    else:\n",
        "        print(f\"\\nMetrics summary not found. Results file exists but metrics may not have been computed.\")\n",
        "        print(\"The evaluation script should have generated metrics during evaluation.\")\n",
        "else:\n",
        "    print(f\"Results file not found at {results_file}\")\n",
        "    print(\"Please run the evaluation step first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
